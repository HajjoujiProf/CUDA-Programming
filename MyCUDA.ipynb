{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HajjoujiProf/CUDA-Programming/blob/main/MyCUDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87rVm726u6Cj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a89d4c9-6f35-422b-bde5-bc04dfd6ad0a"
      },
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1710-9-2-local_9.2.88-1_amd64\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-24 10:09:01--  https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1710-9-2-local_9.2.88-1_amd64\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 152.199.0.24\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|152.199.0.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://developer.nvidia.com/compute/cuda/9.2/prod/local_installers/cuda-repo-ubuntu1710-9-2-local_9.2.88-1_amd64 [following]\n",
            "--2021-02-24 10:09:01--  https://developer.nvidia.com/compute/cuda/9.2/prod/local_installers/cuda-repo-ubuntu1710-9-2-local_9.2.88-1_amd64\n",
            "Reusing existing connection to developer.nvidia.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/9.2/secure/Prod/local_installers/cuda-repo-ubuntu1710-9-2-local_9.2.88-1_amd64.deb?2zsAM9g3wNp-xDnrONksggsVoKBMPpcNzMb8HJ9_45ZUc8nosF4sWTDr_8JoZFzUyR8gSAwrzKx-boCcyW9NxvZEmndniRSelCchl7CUrcvDjgTiDvAdNBWr_1dvL5ORmb_NnRJVGgmEUZjC13rCIo2j9JxeS__LjigzDk8z0MT5nckDP1AO06X9Gbkab9UBX7jxKMY4bjO06qLSgDg [following]\n",
            "--2021-02-24 10:09:02--  https://developer.download.nvidia.com/compute/cuda/9.2/secure/Prod/local_installers/cuda-repo-ubuntu1710-9-2-local_9.2.88-1_amd64.deb?2zsAM9g3wNp-xDnrONksggsVoKBMPpcNzMb8HJ9_45ZUc8nosF4sWTDr_8JoZFzUyR8gSAwrzKx-boCcyW9NxvZEmndniRSelCchl7CUrcvDjgTiDvAdNBWr_1dvL5ORmb_NnRJVGgmEUZjC13rCIo2j9JxeS__LjigzDk8z0MT5nckDP1AO06X9Gbkab9UBX7jxKMY4bjO06qLSgDg\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1269204396 (1.2G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1710-9-2-local_9.2.88-1_amd64’\n",
            "\n",
            "cuda-repo-ubuntu171 100%[===================>]   1.18G   259MB/s    in 4.7s    \n",
            "\n",
            "2021-02-24 10:09:07 (259 MB/s) - ‘cuda-repo-ubuntu1710-9-2-local_9.2.88-1_amd64’ saved [1269204396/1269204396]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lBkcICUvtaX"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN5G8hiOHDgG"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsFer6TyHEZi"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLEtrOllv89b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69af261-368a-4901-8556-1b184b287134"
      },
      "source": [
        "!nvcc --version \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wenZULgywcDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dedc3e63-6c1d-4715-aaa8-6163292b8e9c"
      },
      "source": [
        "\n",
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-6v6sjzh4\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-6v6sjzh4\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp37-none-any.whl size=4307 sha256=a26a58b00fcf45313236f8654095d4e912458090041b756a3884cf1e5124ab7f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9t5vcln1/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQXmsVJWwnVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec750a95-977b-4db5-e5e1-914c5c1d780e"
      },
      "source": [
        "\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEOwF-UCxsdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20703eaa-b9e4-4ea0-8db8-fa42933379e9"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "*c = *a + *b;\n",
        "}\n",
        "int main() {\n",
        "int a, b, c;\n",
        "// host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;\n",
        "// device copies of variables a, b & c\n",
        "int size = sizeof(int);\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "// Setup input values  \n",
        "c = 0;\n",
        "a = 3;\n",
        "b = 14;\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, &a, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_b, &b, size, cudaMemcpyHostToDevice);\n",
        "// Launch add() kernel on GPU\n",
        "add<<<1,1>>>(d_a, d_b, d_c);\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(&c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\"result is %d\\n\",c);\n",
        "// Cleanup\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result is 17\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXp3i2uy570g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e38f6d4-11b8-4103-8474-bf9d69b3b67b"
      },
      "source": [
        "\n",
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "\n",
        "__global__ void print_from_gpu(void) {\n",
        "    printf(\"Hello World! from thread [%d,%d] \\\n",
        "        From device\\n\", threadIdx.x,blockIdx.x);\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "    printf(\"Hello World from host!\\n\");\n",
        "    print_from_gpu<<<1,1>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello World from host!\n",
            "Hello World! from thread [0,0]         From device\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y28Ah9rVHH-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aeb9082-76c8-4d0b-8d9b-069b1261fdb5"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h> \n",
        "\n",
        "int main() {\n",
        "  int nDevices;\n",
        "\n",
        "  cudaGetDeviceCount(&nDevices);\n",
        "  for (int i = 0; i < nDevices; i++) {\n",
        "    cudaDeviceProp prop;\n",
        "    cudaGetDeviceProperties(&prop, i);\n",
        "    printf(\"Device Number: %d\\n\", i);\n",
        "    printf(\"  Device name: %s\\n\", prop.name);\n",
        "    printf(\"  Memory Clock Rate (KHz): %d\\n\",\n",
        "           prop.memoryClockRate);\n",
        "    printf(\"  Memory Bus Width (bits): %d\\n\",\n",
        "           prop.memoryBusWidth);\n",
        "    printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n",
        "           2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "  }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Number: 0\n",
            "  Device name: Tesla T4\n",
            "  Memory Clock Rate (KHz): 5001000\n",
            "  Memory Bus Width (bits): 256\n",
            "  Peak Memory Bandwidth (GB/s): 320.064000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gqStZuSvv-w",
        "outputId": "ebaf5793-fc71-42f3-b543-68e69dbdba95"
      },
      "source": [
        "    %%cu\n",
        "    \n",
        "    #include \"cuda_runtime.h\"\n",
        "    #include \"device_launch_parameters.h\"\n",
        "    #include <stdio.h>\n",
        "    __global__ void HelloWorld()\n",
        "    {\n",
        "        printf(\"Hello world, %d, %d\\n\", blockIdx.x,\n",
        "        threadIdx.x);\n",
        "    }\n",
        "int main()\n",
        "    {\n",
        "        HelloWorld << <2, 5 >> >();\n",
        "    // хост ожидает завершения работы девайса\n",
        "    //the host is waiting for the device to shutdown\n",
        "        cudaDeviceSynchronize();\n",
        "    // ожидаем нажатия любой клавиши\n",
        "    // waiting for any key press\n",
        "        getchar();\n",
        "return 0; }"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello world, 0, 0\n",
            "Hello world, 0, 1\n",
            "Hello world, 0, 2\n",
            "Hello world, 0, 3\n",
            "Hello world, 0, 4\n",
            "Hello world, 1, 0\n",
            "Hello world, 1, 1\n",
            "Hello world, 1, 2\n",
            "Hello world, 1, 3\n",
            "Hello world, 1, 4\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wp-mazymwOK6",
        "outputId": "f836d6a2-5b66-43ef-c2ee-1b170ca7a479"
      },
      "source": [
        "\n",
        "%%cu\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// ядро\n",
        "// core\n",
        "\n",
        "__global__ void add( int *a, int *b, int *c ) {\n",
        "*c = *a + *b; }\n",
        "\n",
        "//главная функция\n",
        "// Main Function\n",
        "\n",
        "int main()\n",
        "{\n",
        "    // переменные на CPU\n",
        "    // variables on the CPU\n",
        " \n",
        "int a, b, c;\n",
        " \n",
        "    // переменные на GPU\n",
        "    // variables on the GPU\n",
        " \n",
        "    int *dev_a, *dev_b, *dev_c;\n",
        "    int size = sizeof( int );\n",
        "//размерность\n",
        " // dimension\n",
        "\n",
        " // выделяем память на GPU\n",
        " // allocate memory to the GPU\n",
        "\n",
        "cudaMalloc( (void**)&dev_a, size );\n",
        "cudaMalloc( (void**)&dev_b, size );\n",
        "cudaMalloc( (void**)&dev_c, size );\n",
        " \n",
        "// инициализация переменных\n",
        " // variable initialization\n",
        "\n",
        "a = 2;\n",
        "b = 10;\n",
        " \n",
        "// копирование информации с CPU на GPU\n",
        " // copying information from CPU to GPU\n",
        "\n",
        "cudaMemcpy( dev_a, &a, size, cudaMemcpyHostToDevice );\n",
        "cudaMemcpy( dev_b, &b, size, cudaMemcpyHostToDevice );\n",
        " \n",
        " // вызов ядра\n",
        " // kernel call\n",
        "\n",
        "add<<< 1, 1 >>>( dev_a, dev_b, dev_c );\n",
        " \n",
        "// копирование результата работы ядра с GPU на CPU\n",
        " // copying the result of the kernel from the GPU to the CPU\n",
        " \n",
        " cudaMemcpy( &c, dev_c, size, cudaMemcpyDeviceToHost );\n",
        "\n",
        "    // вывод информации\n",
        "    // information output\n",
        " \n",
        "    printf(\"%d + %d = %d\\n\", a, b, c);\n",
        " \n",
        "    // очищение памяти на GPU\n",
        "    // clearing memory on GPU\n",
        " \n",
        "    cudaFree( dev_a );\n",
        "    cudaFree( dev_b );\n",
        "    cudaFree( dev_c );\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 + 10 = 12\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFd6LDv-6mKr",
        "outputId": "ab5806e3-914f-47d7-b2a5-caf213a89a65"
      },
      "source": [
        "\n",
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "// Convenience function for checking CUDA runtime API results\n",
        "// can be wrapped around any runtime API call. No-op in release builds.\n",
        "inline\n",
        "cudaError_t checkCuda(cudaError_t result)\n",
        "{\n",
        "#if defined(DEBUG) || defined(_DEBUG)\n",
        "  if (result != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", \n",
        "            cudaGetErrorString(result));\n",
        "    assert(result == cudaSuccess);\n",
        "  }\n",
        "#endif\n",
        "  return result;\n",
        "}\n",
        "\n",
        "void profileCopies(float        *h_a, \n",
        "                   float        *h_b, \n",
        "                   float        *d, \n",
        "                   unsigned int  n,\n",
        "                   char         *desc)\n",
        "{\n",
        "  printf(\"\\n%s transfers\\n\", desc);\n",
        "\n",
        "  unsigned int bytes = n * sizeof(float);\n",
        "\n",
        "  // events for timing\n",
        "  cudaEvent_t startEvent, stopEvent; \n",
        "\n",
        "  checkCuda( cudaEventCreate(&startEvent) );\n",
        "  checkCuda( cudaEventCreate(&stopEvent) );\n",
        "\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  checkCuda( cudaMemcpy(d, h_a, bytes, cudaMemcpyHostToDevice) );\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "\n",
        "  float time;\n",
        "  checkCuda( cudaEventElapsedTime(&time, startEvent, stopEvent) );\n",
        "  printf(\"  Host to Device bandwidth (GB/s): %f\\n\", bytes * 1e-6 / time);\n",
        "\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  checkCuda( cudaMemcpy(h_b, d, bytes, cudaMemcpyDeviceToHost) );\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "\n",
        "  checkCuda( cudaEventElapsedTime(&time, startEvent, stopEvent) );\n",
        "  printf(\"  Device to Host bandwidth (GB/s): %f\\n\", bytes * 1e-6 / time);\n",
        "\n",
        "  for (int i = 0; i < n; ++i) {\n",
        "    if (h_a[i] != h_b[i]) {\n",
        "      printf(\"*** %s transfers failed ***\\n\", desc);\n",
        "      break;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // clean up events\n",
        "  checkCuda( cudaEventDestroy(startEvent) );\n",
        "  checkCuda( cudaEventDestroy(stopEvent) );\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  unsigned int nElements = 4*1024*1024;\n",
        "  const unsigned int bytes = nElements * sizeof(float);\n",
        "\n",
        "  // host arrays\n",
        "  float *h_aPageable, *h_bPageable;   \n",
        "  float *h_aPinned, *h_bPinned;\n",
        "\n",
        "  // device array\n",
        "  float *d_a;\n",
        "\n",
        "  // allocate and initialize\n",
        "  h_aPageable = (float*)malloc(bytes);                    // host pageable\n",
        "  h_bPageable = (float*)malloc(bytes);                    // host pageable\n",
        "  checkCuda( cudaMallocHost((void**)&h_aPinned, bytes) ); // host pinned\n",
        "  checkCuda( cudaMallocHost((void**)&h_bPinned, bytes) ); // host pinned\n",
        "  checkCuda( cudaMalloc((void**)&d_a, bytes) );           // device\n",
        "\n",
        "  for (int i = 0; i < nElements; ++i) h_aPageable[i] = i;      \n",
        "  memcpy(h_aPinned, h_aPageable, bytes);\n",
        "  memset(h_bPageable, 0, bytes);\n",
        "  memset(h_bPinned, 0, bytes);\n",
        "\n",
        "  // output device info and transfer size\n",
        "  cudaDeviceProp prop;\n",
        "  checkCuda( cudaGetDeviceProperties(&prop, 0) );\n",
        "\n",
        "  printf(\"\\nDevice: %s\\n\", prop.name);\n",
        "  printf(\"Transfer size (MB): %d\\n\", bytes / (1024 * 1024));\n",
        "\n",
        "  // perform copies and report bandwidth\n",
        "  profileCopies(h_aPageable, h_bPageable, d_a, nElements, \"Pageable\");\n",
        "  profileCopies(h_aPinned, h_bPinned, d_a, nElements, \"Pinned\");\n",
        "\n",
        "  printf(\"n\");\n",
        "\n",
        "  // cleanup\n",
        "  cudaFree(d_a);\n",
        "  cudaFreeHost(h_aPinned);\n",
        "  cudaFreeHost(h_bPinned);\n",
        "  free(h_aPageable);\n",
        "  free(h_bPageable);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Device: Tesla T4\n",
            "Transfer size (MB): 16\n",
            "\n",
            "Pageable transfers\n",
            "  Host to Device bandwidth (GB/s): 4.856452\n",
            "  Device to Host bandwidth (GB/s): 5.020136\n",
            "\n",
            "Pinned transfers\n",
            "  Host to Device bandwidth (GB/s): 12.165301\n",
            "  Device to Host bandwidth (GB/s): 13.002207\n",
            "n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NNswNJjBDu8"
      },
      "source": [
        "Программа CUDA состоит из двух основных компонентов: хоста и ядра \n",
        "графического процессора.\n",
        "Глобальная память - это память с высокой задержкой\n",
        " cudaMalloc() "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbiPziGtQLBp",
        "outputId": "bc9655db-1932-4c6a-ac9e-72980306c6a4"
      },
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <math.h> \n",
        "#include <curand.h>\n",
        "#include <curand_kernel.h>\n",
        "\n",
        "#define NUM_BLOCK  1024\n",
        "#define NUM_THREAD 1024\n",
        "\n",
        "#define CUDA_CALL(x) do { if ((x) != cudaSuccess ) { printf (\" Error at %s :%d\\n - %s\" , __FILE__ , __LINE__ , cudaGetErrorString(cudaGetLastError())) ; return EXIT_FAILURE ;}} while (0)\n",
        "#define CURAND_CALL(x) do { if ((x) != CURAND_STATUS_SUCCESS ) { printf (\" Error at %s :%d\\n - %s\" , __FILE__ , __LINE__, cudaGetErrorString(cudaGetLastError())) ; return EXIT_FAILURE ;}} while (0)\n",
        "\n",
        "__global__ void checkIsInCircle(int *sum, float *randomX, float *randomY) {\n",
        "\n",
        "\tint id = blockIdx.x * NUM_BLOCK + threadIdx.x;\n",
        "\n",
        "\tfloat px = randomX[id];\n",
        "\tfloat py = randomY[id];\n",
        "\n",
        "\tif (px * px + py * py <= 1) {\n",
        "\t\tatomicAdd(&sum[blockIdx.x], 1);\t \t\n",
        "\t}\n",
        "}\n",
        "\n",
        "\n",
        "int main(void) {\n",
        "\tint *sumHost, *sumDev;\n",
        "\tsize_t sumSize = NUM_BLOCK * sizeof(int);\n",
        "\tCUDA_CALL(cudaMalloc((void **) &sumDev, sumSize));\n",
        "\tCUDA_CALL(cudaMemset(sumDev, 0, sumSize));\n",
        "\tsumHost = (int *)malloc(sumSize);\n",
        "\n",
        "\tfloat *randomX, *randomY;\n",
        "\tsize_t randomSize = NUM_BLOCK * NUM_THREAD * sizeof(float);\n",
        "\tCUDA_CALL(cudaMalloc((void **) &randomX, randomSize));\n",
        "\tCUDA_CALL(cudaMalloc((void **) &randomY, randomSize));\n",
        "\t\t\n",
        "\tcurandGenerator_t generator;\n",
        "\n",
        "\tCURAND_CALL(curandCreateGenerator(&generator, CURAND_RNG_PSEUDO_DEFAULT)); \n",
        "\n",
        "\tcudaEvent_t start, stop;\n",
        "\tfloat mean_time = 0;\n",
        "\tfloat elapsedTime;\n",
        "\n",
        "\tint *allSum = (int *)malloc(NUM_BLOCK * sizeof(int));\n",
        "\tmemset(allSum, 0, NUM_BLOCK * sizeof(int));\n",
        "\n",
        "\tint iterationsCount = 1000;\n",
        "\n",
        "\tfor (int i = 0; i < iterationsCount; ++i)\n",
        "\t{\n",
        "\t\tcudaEventCreate(&start);\n",
        "\t\tcudaEventCreate(&stop);\n",
        "\t\tcudaEventRecord(start, 0);\n",
        "\n",
        "\n",
        "\t\tCURAND_CALL(curandGenerateUniform(generator, randomX, NUM_BLOCK * NUM_THREAD));\n",
        "\t\tCURAND_CALL(curandGenerateUniform(generator, randomY, NUM_BLOCK * NUM_THREAD)); \n",
        "\n",
        "\t\tcheckIsInCircle <<<NUM_BLOCK, NUM_THREAD>>> (sumDev, randomX, randomY); \n",
        "\t\n",
        "\t\tCUDA_CALL(cudaMemcpy(sumHost, sumDev, sumSize, cudaMemcpyDeviceToHost));\n",
        "\n",
        "\t\tcudaEventRecord(stop, 0);\n",
        "\t\tcudaEventSynchronize(stop);\n",
        "\t\tcudaEventElapsedTime(&elapsedTime, start, stop);\n",
        "\t\tmean_time += elapsedTime;\n",
        "\n",
        "\n",
        "\t\tfor (int i = 0; i < NUM_BLOCK; ++i)\n",
        "\t\t{\n",
        "\t\t\tallSum[i] += sumHost[i];\n",
        "\t\t}\n",
        "\n",
        "\t\tCUDA_CALL(cudaMemset(sumDev, 0, sumSize));\n",
        "\n",
        "\t\tcudaEventDestroy(start);\n",
        "\t\tcudaEventDestroy(stop);\n",
        "\t}\n",
        "\n",
        "\tprintf(\"\\nResult %fms\\n\", mean_time / iterationsCount);\n",
        "\n",
        "\tfloat count = 0;\n",
        "\tfor(int tid = 0; tid < NUM_BLOCK; tid++) {\n",
        "\t\tcount += (float) allSum[tid] / iterationsCount;\n",
        "\t}\n",
        "\n",
        "\tprintf(\"\\n%f vs %d\", count, NUM_BLOCK * NUM_THREAD);\n",
        "\t\n",
        "\tfloat pi = (count / (NUM_BLOCK * NUM_THREAD)) * 4;\n",
        "\n",
        "\tprintf(\"\\nPI = %f\\n\", pi);\n",
        "\n",
        "\n",
        "\tfree(sumHost); \n",
        "\tcudaFree(sumDev);\n",
        "\tcudaFree(randomX);\n",
        "\tcudaFree(randomY);\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tmp/tmpxft_000000c0_00000000-10_ea559df7-6a2a-4dc2-9509-0029fba945ae.o: In function `main':\n",
            "tmpxft_000000c0_00000000-5_ea559df7-6a2a-4dc2-9509-0029fba945ae.cudafe1.cpp:(.text+0x1a7): undefined reference to `curandCreateGenerator'\n",
            "tmpxft_000000c0_00000000-5_ea559df7-6a2a-4dc2-9509-0029fba945ae.cudafe1.cpp:(.text+0x277): undefined reference to `curandGenerateUniform'\n",
            "tmpxft_000000c0_00000000-5_ea559df7-6a2a-4dc2-9509-0029fba945ae.cudafe1.cpp:(.text+0x2ce): undefined reference to `curandGenerateUniform'\n",
            "collect2: error: ld returned 1 exit status\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qViRBtLKjBFs",
        "outputId": "f62ca36a-20a9-4f69-add6-48b934a95e44"
      },
      "source": [
        "\n",
        "\n",
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "//Elemetwise multiplicaton of two vectors // \n",
        "__global__ void kMartixByMatrixElementwise(const int nThreads, const float *m1, const float *m2, float *output) {\n",
        "    /*  Computes the product of two arrays (elementwise multiplication).\n",
        "     Inputs:\n",
        "     m1: array\n",
        "     m2: array\n",
        "     output: array,the results of the multiplication are to be stored here\n",
        "    */\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = m1[i] * m2[i];\n",
        "\t  }\n",
        "}\n",
        "\n",
        "\n",
        "//Elemetwise multiplicaton of two vectors Device //\n",
        "__device__ float* dMartixByMatrixElementwise(const float *m1, const float *m2, float *output, const int width, const int height){\n",
        "\n",
        "\tkMartixByMatrixElementwise <<< width, height >>> ( width * height, m1, m2, output );\n",
        "    cudaDeviceSynchronize();\n",
        "    return output;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "//CUDA kernel which finds the difference between two matrices //\n",
        "\n",
        "__global__ void kMartixSubstractMatrix(const int nThreads, const float *m1, const float *m2, float *output) {\n",
        "    /*  Computes the (elementwise) difference between two arrays\n",
        "     Inputs:\n",
        "     m1: array\n",
        "     m2: array\n",
        "     output: array,the results of the computation are to be stored here\n",
        "     */\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = m1[i] - m2[i];\n",
        "\t  }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "//\n",
        "__device__ float* dMartixSubstractMatrix(const float *m1, const float *m2, float *output, const int width, const int height){\n",
        "\n",
        "\tkMartixSubstractMatrix <<< width, height >>> ( width * height, m1, m2, output );\n",
        "    cudaDeviceSynchronize();\n",
        "    return output;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "//sigmoid function definition //\n",
        "\n",
        "__global__ void kSigmoid(const int nThreads, float const *input, float *output){\n",
        "    /*  Computes the value of the sigmoid function f(x) = 1/(1 + e^-x).\n",
        "     Inputs:\n",
        "     input: array\n",
        "     output: array, the results of the computation are to be stored here\n",
        "    */\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = 1.0 / (1.0 + std::exp(-input[i]));\n",
        "\t  }\n",
        "}\n",
        "\n",
        "__device__ void dSigmoid(float const *input, float *output, const int height, const int width){\n",
        "\n",
        "\tkSigmoid <<< height, width >>> (height * width, input, output);\n",
        "\tcudaDeviceSynchronize();\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "//Finding the values of the hidden layer function //\n",
        "\n",
        "__global__ void kSigmoid_d(const int nThreads, float const *input, float *output) {\n",
        "\t/*  Computes the value of the sigmoid function derivative f'(x) = f(x)(1 - f(x)),\n",
        "\t    where f(x) is sigmoid function.\n",
        "\t    Inputs:\n",
        "\t    input: array\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tx(1 - x) for every element of the input matrix m1.\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = input[i] * (1 - input[i]);\n",
        "\t  }\n",
        "}\n",
        "\n",
        "__device__ float* dSigmoid_d(float const *input, float *output, const int rows, const int columns){\n",
        "\tkSigmoid_d <<< rows, columns >>> (rows*columns, input, output);\n",
        "\tcudaDeviceSynchronize();\n",
        "\treturn output;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "//matrix multiplication function definition //\n",
        "\n",
        "__global__ void kDot(const int nThreads, const float *m1, const float *m2, float *output, const int m1_rows , const int m1_columns, const int m2_columns ){\n",
        "\t/*  Computes the product of two matrices: m1 x m2.\n",
        "\t   \tInputs:\n",
        "\t    m1: array, left matrix of size m1_rows x m1_columns\n",
        "\t    m2: array, right matrix of size m1_columns x m2_columns (the number of rows in the right matrix\n",
        "\t    must be equal to the number of the columns in the left one)\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tm1 * m2, product of two arrays m1 and m2, a matrix of size m1_rows x m2_columns\n",
        "\t    m1_rows: int, number of rows in the left matrix m1\n",
        "\t    m1_columns: int, number of columns in the left matrix m1\n",
        "\t    m2_columns: int, number of columns in the right matrix m2\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t{\n",
        "\t    int r = (int)i / m2_columns;\n",
        "\t    int c = i % m2_columns;\n",
        "\t    float t_output = 0.f;\n",
        "\n",
        "\t    for( int k = 0; k < m1_columns; ++k ) {\n",
        "\t        t_output += m1[ r * m1_columns + k ] * m2[ k * m2_columns + c ];\n",
        "\t    }\n",
        "\n",
        "\t    output[i] = t_output;\n",
        "\t}\n",
        "}\n",
        "\n",
        "__device__ float* dDot(const float *m1, const float *m2, float *output, const int m1_rows , const int m1_columns, const int m2_columns ){\n",
        "\n",
        "\tkDot <<< m1_rows, m2_columns >>> (m1_rows * m2_columns, m1, m2, output, m1_rows , m1_columns, m2_columns );\n",
        "\tcudaDeviceSynchronize();\n",
        "\treturn output;\n",
        "}\n",
        "\n",
        "__global__ void kDot_m1_m2T(const int nThreads, const float *m1, const float *m2, float *output, const int m1_columns, const int m2_rows ){\n",
        "\t/*  Updates the output matrix with the product of two matrices: m1 and m2 transposed.\n",
        "\t   \tInputs:\n",
        "\t    m1: array, left matrix of size m1_rows x m1_columns\n",
        "\t    m2: array, right matrix of size m2_rows x m1_columns (m2 transposed will be of size m1_columns x m2_rows)\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tm1 * m2, product of two arrays m1 and m2, a matrix of size m1_rows x m2_rows\n",
        "\t    m1_columns: int, number of columns in the left matrix m1\n",
        "\t    m2_rows: int, number of rows in the left matrix m2\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t{\n",
        "\t\tint r = (int)i / m2_rows;\n",
        "\t\tint c = i % m2_rows;\n",
        "\t\tfloat t_output = 0.0;\n",
        "\t\tint id_T;\n",
        "\n",
        "\t\tfor( int k = 0; k < m1_columns; ++k ) {\n",
        "\t\t\tid_T = c * m1_columns + k;\n",
        "\t\t\tt_output += m1[ r * m1_columns + k ] * m2[ id_T ];\n",
        "\t\t}\n",
        "\n",
        "\t\toutput[i] = t_output;\n",
        "\t}\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "//subroutine that multiplies matrix by transposed matrix //\n",
        "\n",
        "__device__ float* dDot_m1_m2T(const float *m1, const float *m2, float *output, const int m1_rows , const int m1_columns, const int m2_rows )\n",
        "{\n",
        "\tkDot_m1_m2T <<< m1_rows, m2_rows >>> ( m1_rows * m2_rows, m1, m2, output, m1_columns, m2_rows );\n",
        "\tcudaDeviceSynchronize();\n",
        "\treturn output;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "// subroutine that multiplies matrix by transposed matrix kernel  //\n",
        "\n",
        "__global__ void kDot_m1T_m2(const int nThreads, const float *m1, const float *m2, float *output, const int m1_rows,\n",
        "\t\t\t\t\t\t\tconst int m1_columns, const int m2_columns ){\n",
        "\t/*  Increments the output matrix with the product of two matrices: m1 transposed and m2.\n",
        "\t   \tInputs:\n",
        "\t    m1: array, left matrix of size m1_rows x m1_columns (m1 transposed will be of size m1_columns x m1_rows)\n",
        "\t    m2: array, right matrix of size m1_rows x m2_columns\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tm1 * m2, product of two arrays m1 and m2, a matrix of size m1_columns x m2_columns\n",
        "\t    m1_rows: int, number of rows in the left matrix m1\n",
        "\t    m1_columns: int, number of columns in the left matrix m1\n",
        "\t    m2_rows: int, number of rows in the left matrix m2\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t{\n",
        "\t    int r = (int)i / m2_columns;\n",
        "\t    int c = i % m2_columns;\n",
        "\t    int id_T;\n",
        "\t    float t_output = 0.0;\n",
        "\n",
        "\t    for( int k = 0; k < m1_rows; ++k ) {\n",
        "\t    \tid_T = k * m1_columns + r;\n",
        "\t        t_output += m1[ id_T ] * m2[ k * m2_columns + c ];\n",
        "\t    }\n",
        "\n",
        "\t    output[i] += t_output;\n",
        "\t}\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "__device__ void dDot_m1T_m2(const float *m1, const float *m2, float *output, const int m1_height , const int m1_width, const int m2_width )\n",
        "{\n",
        "\tkDot_m1T_m2 <<< m1_width, m2_width >>> (m1_width * m2_width, m1, m2, output, m1_height, m1_width, m2_width );\n",
        "\tcudaDeviceSynchronize();\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "__device__ void kPrintMatrix (const float* M, int h, int w) {\n",
        "    /*  Prints out the input array as h x w matrix.\n",
        "     Inputs:\n",
        "     m: vector, matrix of size n_rows x n_columns\n",
        "     h: int, number of rows in the matrix M\n",
        "     w: int, number of columns in the matrix M\n",
        "     */\n",
        "\tfor (int i = 0; i < h; i++){\n",
        "\t\tfor (int j = 0; j < w; j++){\n",
        "\t\t\tprintf(\"%f  \", M[i*w+j]);\n",
        "\t\t}\n",
        "\t\tprintf(\"\\n\");\n",
        "\t}\n",
        "\tprintf(\"\\n\");\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "// The loop runs for 50 iterations (epochs) and fits the vector of attributes X to the vector of classes y //\n",
        "\n",
        "\n",
        "__global__ void kFit(\tconst float* X, const int X_w, const int X_h,\n",
        "\t\t\t\t\t\tconst float* y, const int y_w,\n",
        "\t\t\t\t\t\tfloat* l1, const int l1_w, float* l_1_d,\n",
        "\t\t\t\t\t\tfloat* pred, float* pred_d,\n",
        "\t\t\t\t\t\tfloat* W0,\n",
        "\t\t\t\t\t\tfloat* W1,\n",
        "\t\t\t\t\t\tfloat* buffer\n",
        "\t\t\t\t\t\t)\n",
        "{\n",
        "    \n",
        "   \n",
        "\tfor (unsigned i = 0; i < 50; ++i) {\n",
        "\n",
        "        dSigmoid(dDot(X, W0, l1, X_h, X_w, l1_w), l1, X_h, l1_w);\n",
        "        dSigmoid(dDot(l1, W1, pred, X_h, l1_w, y_w), pred, X_h, y_w);\n",
        "        dMartixByMatrixElementwise(dMartixSubstractMatrix(y, pred, pred_d, X_h, y_w), dSigmoid_d(pred, buffer, X_h, y_w), pred_d, X_h, y_w );\n",
        "        dMartixByMatrixElementwise(dDot_m1_m2T(pred_d, W1, l_1_d, X_h, y_w, l1_w), dSigmoid_d(l1, buffer, X_h, l1_w), l_1_d, X_h, l1_w);\n",
        "        dDot_m1T_m2( l1, pred_d, W1, X_h, l1_w, y_w );\n",
        "        dDot_m1T_m2( X, l_1_d, W0, X_h, X_w, l1_w );\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "// A Layer //\n",
        "\n",
        "int main(void){\n",
        "    \n",
        "    //h_ stands for hostss (CPU) //\n",
        "    //d_ stands for device  ( GPU )//\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "// define hyperparameters //\n",
        "\n",
        "\tconst int TRAINING_SIZE = 4;\n",
        "\tconst int TRAINING_DIM = 4;\n",
        "\tconst int L1_SIZE = 8;\n",
        "\n",
        "\t// X, the first 4 lines from Iris dataset\n",
        "\tfloat h_X[TRAINING_SIZE*TRAINING_DIM] = {\t5.1, 3.5, 1.4, 0.2,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t4.9, 3.0, 1.4, 0.2,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t6.2, 3.4, 5.4, 2.3,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t5.9, 3.0, 5.1, 1.8 };\n",
        "\n",
        "\n",
        "\n",
        "//define inpute size //\n",
        "\tconst signed int X_size = sizeof(h_X);\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "//allocate space on GPU for input data //\n",
        "\tfloat *d_X;\n",
        "\tcudaMalloc(&d_X, X_size);\n",
        "\tcudaMemcpy(d_X, h_X, X_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "\n",
        "// we intilize the first wheight matrix randomly //\n",
        "\t//WEIGHTS_0\n",
        "\tconst long signed int W0_size = L1_SIZE*TRAINING_DIM*sizeof(float);\n",
        "\tfloat *h_W0 = (float*)malloc(W0_size);\n",
        "\tfor (int i = 0; i < L1_SIZE*TRAINING_DIM; i++){\n",
        "\t    h_W0[i] = 0.1 * (2.0*rand()/RAND_MAX-1.0);\n",
        "\t}\n",
        "\n",
        "\n",
        "\n",
        "// we allocate space on GPU for the weights //\n",
        "\n",
        "\tfloat *d_W0;\n",
        "\tcudaMalloc(&d_W0, W0_size);\n",
        "\tcudaMemcpy(d_W0, h_W0, W0_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // layer first size //\n",
        "\n",
        "\t//LAYER_1, LAYER_1_DELTA AND BUFFER OF LAYER 1 SIZE\n",
        "\tconst long signed int L1_size = L1_SIZE*TRAINING_SIZE*sizeof(float);\n",
        "\n",
        "\n",
        "  //define layer 1 variables ( delta and buffer ) // \n",
        "\n",
        "\tfloat* h_layer_1 = (float*)malloc(L1_size);\n",
        "\tfloat* h_layer_1_delta = (float*)malloc(L1_size);\n",
        "\tfloat* h_buffer = (float*)malloc(L1_size);\n",
        "\n",
        "\n",
        "  // give layer 1 initial 0 values // \n",
        "\n",
        "\tfor (int i = 0; i < L1_SIZE*TRAINING_SIZE; i++){\n",
        "\t    h_layer_1[i] = 0.0;\n",
        "\t    h_buffer[i] = 0.0;\n",
        "\t    h_layer_1_delta[i] = 0.0;\n",
        "\t}\n",
        "\n",
        "\n",
        "\n",
        "//allocate GPU space for Layer 1 // \n",
        "\tfloat *d_layer_1;\n",
        "\tcudaMalloc(&d_layer_1, L1_size);\n",
        "\tcudaMemcpy(d_layer_1, h_layer_1, L1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "//allocate GPU space for Layer 1 buffer // \n",
        "\tfloat *d_buffer;\n",
        "\tcudaMalloc(&d_buffer, L1_size);\n",
        "\tcudaMemcpy(d_buffer, h_buffer, L1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "\n",
        "//allocate GPU space for Layer 1 delta //\n",
        "\tfloat *d_layer_1_delta;\n",
        "\tcudaMalloc(&d_layer_1_delta, L1_size);\n",
        "\tcudaMemcpy(d_layer_1_delta, h_layer_1_delta, L1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "\n",
        "//allocated weight randomly 1 //\n",
        "\t//WEIGHTS_1\n",
        "\tconst long signed int W1_size = L1_SIZE*sizeof(float);\n",
        "\tfloat *h_W1 = (float*)malloc(W1_size);\n",
        "\tfor (int i = 0; i < L1_SIZE; i++){\n",
        "\t    h_W1[i] = 0.1* (2.0*rand()/RAND_MAX-1.0);\n",
        "\t}\n",
        "\n",
        "\tfloat *d_W1;\n",
        "\tcudaMalloc(&d_W1, W1_size);\n",
        "\tcudaMemcpy(d_W1, h_W1, W1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "  //allocate GPU space for weiht 1 //\n",
        "\n",
        "\t//Y\n",
        "\tfloat h_y[4] = {\t0,\n",
        "\t\t\t\t\t\t0,\n",
        "\t\t\t\t\t\t1,\n",
        "\t\t\t\t\t\t1 };\n",
        "\n",
        "\n",
        "   //allocate GPU for labels //         \n",
        "\tconst signed int y_size = sizeof(h_y);\n",
        "\tfloat *d_y;\n",
        "\tcudaMalloc(&d_y, y_size);\n",
        "\tcudaMemcpy(d_y, h_y, y_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "\n",
        "// define the prediction and prediction delta //\n",
        "\t//PRED AND PRED_DELTA\n",
        "\tfloat* h_pred = (float*)malloc(y_size);\n",
        "\tfloat* h_pred_delta = (float*)malloc(y_size);\n",
        "\tfor (int i = 0; i < TRAINING_SIZE; i++){\n",
        "\t    h_pred[i] = 0.0;\n",
        "\t    h_pred_delta[i] = 0.0;\n",
        "\t}\n",
        "\n",
        "\n",
        "//allocate GPU space for prediction //\n",
        "\tfloat *d_pred;\n",
        "\tcudaMalloc(&d_pred, y_size);\n",
        "\tcudaMemcpy(d_pred, h_pred, y_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "//allocate GPU space for prediction delta // \n",
        "\tfloat *d_pred_delta;\n",
        "\tcudaMalloc(&d_pred_delta, y_size);\n",
        "\tcudaMemcpy(d_pred_delta, h_pred_delta, y_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\tkFit <<< 1, 1 >>> (\td_X, TRAINING_DIM, TRAINING_SIZE,\n",
        "\t\t\t\t\t\td_y, 1,\n",
        "\t\t\t\t\t\td_layer_1, L1_SIZE, d_layer_1_delta,\n",
        "\t\t\t\t\t\td_pred,\n",
        "\t\t\t\t\t\td_pred_delta,\n",
        "\t\t\t\t\t\td_W0,\n",
        "\t\t\t\t\t\td_W1,\n",
        "\t\t\t\t\t\td_buffer);\n",
        "\n",
        "\tcudaMemcpy(h_pred, d_pred, y_size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "//free GPU memory //\n",
        "\tcudaFree(d_pred);\n",
        "\tcudaFree(d_X);\n",
        "\tcudaFree(d_y);\n",
        "\tcudaFree(d_layer_1_delta);\n",
        "\tcudaFree(d_pred_delta);\n",
        "\tcudaFree(d_W0);\n",
        "\tcudaFree(d_W1);\n",
        "\tcudaFree(d_buffer);\n",
        "\n",
        "\n",
        "\n",
        "  // free CPU Memory //\n",
        "\n",
        "\tfree(h_layer_1_delta);\n",
        "\tfree(h_pred_delta);\n",
        "\tfree(h_W0);\n",
        "\tfree(h_W1);\n",
        "\tfree(h_buffer);\n",
        "\n",
        "  //print all the predictions //\n",
        "\n",
        "\tfor (int i = 0; i < TRAINING_SIZE; i++){\n",
        "\t\tprintf(\"Prediction[%i] : %f True Value[%i] : %f Error[%i] : %f\\n\", i, h_pred[i], i, h_y[i], i, h_pred[i] - h_y[i]);\n",
        "\t}\n",
        "\n",
        "\tfree(h_pred);\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tmp/tmpx_pz9wd8/c7e452ef-ea0c-44bc-94d4-b71d2245fa77.cu(26): error: calling a __global__ function(\"kMartixByMatrixElementwise\") from a __device__ function(\"dMartixByMatrixElementwise\") is only allowed on the compute_35 architecture or above\n",
            "\n",
            "/tmp/tmpx_pz9wd8/c7e452ef-ea0c-44bc-94d4-b71d2245fa77.cu(59): error: calling a __global__ function(\"kMartixSubstractMatrix\") from a __device__ function(\"dMartixSubstractMatrix\") is only allowed on the compute_35 architecture or above\n",
            "\n",
            "/tmp/tmpx_pz9wd8/c7e452ef-ea0c-44bc-94d4-b71d2245fa77.cu(87): error: calling a __global__ function(\"kSigmoid\") from a __device__ function(\"dSigmoid\") is only allowed on the compute_35 architecture or above\n",
            "\n",
            "/tmp/tmpx_pz9wd8/c7e452ef-ea0c-44bc-94d4-b71d2245fa77.cu(113): error: calling a __global__ function(\"kSigmoid_d\") from a __device__ function(\"dSigmoid_d\") is only allowed on the compute_35 architecture or above\n",
            "\n",
            "/tmp/tmpx_pz9wd8/c7e452ef-ea0c-44bc-94d4-b71d2245fa77.cu(153): error: calling a __global__ function(\"kDot\") from a __device__ function(\"dDot\") is only allowed on the compute_35 architecture or above\n",
            "\n",
            "/tmp/tmpx_pz9wd8/c7e452ef-ea0c-44bc-94d4-b71d2245fa77.cu(194): error: calling a __global__ function(\"kDot_m1_m2T\") from a __device__ function(\"dDot_m1_m2T\") is only allowed on the compute_35 architecture or above\n",
            "\n",
            "/tmp/tmpx_pz9wd8/c7e452ef-ea0c-44bc-94d4-b71d2245fa77.cu(239): error: calling a __global__ function(\"kDot_m1T_m2\") from a __device__ function(\"dDot_m1T_m2\") is only allowed on the compute_35 architecture or above\n",
            "\n",
            "7 errors detected in the compilation of \"/tmp/tmpxft_00000349_00000000-8_c7e452ef-ea0c-44bc-94d4-b71d2245fa77.cpp1.ii\".\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h82Uw48QiEM3"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QgnTGUshp1W",
        "outputId": "13c2efa7-6ae1-433c-eabb-571706a11a54"
      },
      "source": [
        "\n",
        "\n",
        "%%cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <time.h>\n",
        "#include <random>\n",
        "#include <curand.h>\n",
        "#include <math.h>\n",
        "#include \"kernels.cuh\"\n",
        "\n",
        "\n",
        "\n",
        "int main()\n",
        "{\n",
        "\tunsigned int n = 256*256;\n",
        "\tunsigned int m = 20000;\n",
        "\tint *h_count;\n",
        "\tint *d_count;\n",
        "\tcurandState *d_state;\n",
        "\tfloat pi;\n",
        "\n",
        "\n",
        "\t// allocate memory\n",
        "\th_count = (int*)malloc(n*sizeof(int));\n",
        "\tcudaMalloc((void**)&d_count, n*sizeof(int));\n",
        "\tcudaMalloc((void**)&d_state, n*sizeof(curandState));\n",
        "\tcudaMemset(d_count, 0, sizeof(int));\n",
        "\n",
        "\n",
        "\t// set up timing stuff\n",
        "\tfloat gpu_elapsed_time;\n",
        "\tcudaEvent_t gpu_start, gpu_stop;\n",
        "\tcudaEventCreate(&gpu_start);\n",
        "\tcudaEventCreate(&gpu_stop);\n",
        "\tcudaEventRecord(gpu_start, 0);\n",
        "\n",
        "\n",
        "\t// set kernel\n",
        "\tdim3 gridSize = 256;\n",
        "\tdim3 blockSize = 256;\n",
        "\tsetup_kernel<<< gridSize, blockSize>>>(d_state);\n",
        "\n",
        "\n",
        "\t// monti carlo kernel\n",
        "\tmonti_carlo_pi_kernel<<<gridSize, blockSize>>>(d_state, d_count, m);\n",
        "\n",
        "\n",
        "\t// copy results back to the host\n",
        "\tcudaMemcpy(h_count, d_count, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\tcudaEventRecord(gpu_stop, 0);\n",
        "\tcudaEventSynchronize(gpu_stop);\n",
        "\tcudaEventElapsedTime(&gpu_elapsed_time, gpu_start, gpu_stop);\n",
        "\tcudaEventDestroy(gpu_start);\n",
        "\tcudaEventDestroy(gpu_stop);\n",
        "\n",
        "\n",
        "\t// display results and timings for gpu\n",
        "\tpi = *h_count*4.0/(n*m);\n",
        "\tstd::cout<<\"Approximate pi calculated on GPU is: \"<<pi<<\" and calculation took \"<<gpu_elapsed_time<<std::endl;\n",
        "\n",
        "\n",
        "\t//  serial verion\n",
        "\tclock_t cpu_start = clock();\n",
        "\tstd::default_random_engine generator;\n",
        "\tstd::uniform_real_distribution<float> distribution(0, 1.0);\n",
        "\tint count = 0;\n",
        "\tfor(unsigned int i=0;i<n;i++){\n",
        "\t\tint temp = 0;\n",
        "\t\twhile(temp < m){\n",
        "\t\t\tfloat x = distribution(generator);\n",
        "\t\t\tfloat y = distribution(generator);\n",
        "\t\t\tfloat r = x*x + y*y;\n",
        "\n",
        "\t\t\tif(r <= 1){\n",
        "\t\t\t\tcount++;\n",
        "\t\t\t}\n",
        "\t\t\ttemp++; \n",
        "\t\t}\n",
        "\t}\n",
        "\tclock_t cpu_stop = clock();\n",
        "\tpi = 4.0*count/(n*m);\n",
        "\tstd::cout<<\"Approximate pi calculated on CPU is: \"<<pi<<\" and calculation took \"<<1000*(cpu_stop - cpu_start)/CLOCKS_PER_SEC<<std::endl;\n",
        "\n",
        "\n",
        "\n",
        "\t// delete memory\n",
        "\tfree(h_count);\n",
        "\tcudaFree(d_count);\n",
        "\tcudaFree(d_state);\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tmp/tmpw0fgknq9/9bc83706-c9a8-43a6-b2db-b25388d65447.cu:7:10: fatal error: kernels.cuh: No such file or directory\n",
            " #include \"kernels.cuh\"\n",
            "          ^~~~~~~~~~~~~\n",
            "compilation terminated.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}